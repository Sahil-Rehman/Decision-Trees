{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Decision Trees and Ensemble Methods [ __ / 70  marks]\n",
    "\n",
    "<img src=\"https://datasciencetoday.net/images/2018/11/27/tree.png\">\n",
    "\n",
    "For this assignment we will use standalone and ensembled decision trees (Bagging, AdaBoost) in order to predict whether particular red wines are `high quality` or `low quality` based on some associated input features (e.g., fixed acidity, residual sugar, density, alcohol, etc). \n",
    "\n",
    "We will first import our data. Next, we will apply the pre-processing steps. Finally, we will construct and compare models. There will also be some communication questions along the way. \n",
    "\n",
    "I hope this week's lesson and this assignment will make you more comfortable with using and thinking about decision trees --- they're a very powerful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start...\n",
    "* check out the relevant lecture code for reference (`L9_CF.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you submit...\n",
    "* restart the kernel, then re-run the whole notebook to ensure no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 [ _ /3 marks]\n",
    "\n",
    "Read the file `winequality-red.csv` into a pandas DataFrame. Display the first 5 rows of the DataFrame. Make sure to remove the semicolons.\n",
    "\n",
    "`Hint`: Sometimes you will see that dataset entries are separated by `;`,`,`,`&`. You can use the `sep` argument in `read_csv()` to format it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ****** your code here ******\n",
    "df=pd.read_csv(\"winequality-red.csv\",sep=\";\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 [ _ /7 marks]\n",
    "\n",
    "Before building our models, we will need to **preprocess** the data. Instead of using the 10-class column ('quality') directly, let's just focus on classifying red wines as `'high quality'` or `'low quality'` by manually assigning a threshold. We will consider wines with 'quality' 7 or higher as **'high quality'** (class label `1`) and those with 'quality' 6 or lower as **'low quality'** (class label `0`). Replace the `quality` column with your new column (`CLASS`). \n",
    "\n",
    "Display the first 5 rows of the new dataframe. How many instances of class `0` and class `1` are there? [ /1 mark] Is the data class-balanced? [ /1 mark] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  CLASS  \n",
       "0      9.4      0  \n",
       "1      9.8      0  \n",
       "2      9.8      0  \n",
       "3      9.8      0  \n",
       "4      9.4      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1382\n",
       "1     217\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace dependent variable 'quality' with `CLASS` (labels 0 and 1) [ /4 marks]\n",
    "# ****** your code here ******\n",
    "df=df.rename(columns={\"quality\": \"CLASS\"})\n",
    "df.head()\n",
    "\n",
    "df.loc[(df.CLASS <= 6),'CLASS']=0\n",
    "df.loc[(df.CLASS >= 7),'CLASS']=1\n",
    "display(df.head())\n",
    "# How many instances of class 0 and class 1 are there in the data? (code) [ /1 mark]\n",
    "# ****** your code here ******\n",
    "df.CLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**:There are 1382 instances of 0 and 217 instances of 1, therefore the data class is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 [ _ /3 marks]\n",
    "\n",
    "Let's create our train and test sets. Create an input dataframe X (the input features); next, create an output series y which contains the output class labels (a column of `0`'s and `1`'s). Split the data into train and test sets with `train_test_split`. Use `test_size = 0.3`, `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature data into X; store the class data into y [ /2 marks]\n",
    "# ****** your code here ******\n",
    "X=df.drop(\"CLASS\",axis=1)\n",
    "y=df.CLASS\n",
    "\n",
    "# Do a train-test split. Use 30% of the data for testing. Use random state 0. [ /1 mark]\n",
    "# ****** your code here ******\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 [ _ /13 marks]\n",
    "\n",
    "For our first model we will **select a standalone decision tree of optimal maximum depth** (out of possible maximum depths ranging from `2 to 30`). \n",
    "\n",
    "To find the optimal maximum depth, create multiple decision trees with sklearn's `DecisionTreeClassifier` class (you can use a loop), then compute the mean Cross-Validation score for each (use 5-fold CV). You don't need to specify a scorer (Note: for `DecisionTreeClassifier` it's *mean accuracy*).\n",
    "\n",
    "Create a plot which shows the mean CV scores on the y-axis and maximum depths on the x-axis. **Report the optimal maximum depth**. [ /2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CV Score')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and fit trees from max_depth 2 to max_depth 30. Use 5-fold CV for each. [ /8 marks]\n",
    "# ****** your code here ******\n",
    "mcvs=[]\n",
    "for i in range(2,31):\n",
    "    tree = DecisionTreeClassifier(max_depth=i,random_state=0)\n",
    "    mcvs.append(cross_val_score(tree, Xtrain, ytrain, cv=5).mean())\n",
    "\n",
    "\n",
    "\n",
    "# Plot the mean CV score vs. maximum depth [ /3 marks]\n",
    "# ****** your code here ******\n",
    "max_depth=list(range(2,31))\n",
    "plt.plot(max_depth,mcvs)\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "\n",
    "\n",
    "# Report the optimal max_depth either here or in the markdown cell below. [ /2 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: The optimal maximum depth is 4 since at depth 4, it has maximum CV score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 [ _ /6 marks]\n",
    "\n",
    "Consider your optimal max_depth tree (or, you can create a new one with that depth). Fit it to the training set. Report its test accuracy on the test set.\n",
    "\n",
    "Hint: You can use `accuracy_score()` here to report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your optimal-depth tree, then calculate test accuracy [ /6 marks]\n",
    "# ****** your code here ******\n",
    "tree=DecisionTreeClassifier(max_depth=4,random_state=0)\n",
    "tree.fit(Xtrain,ytrain)\n",
    "ypred=tree.predict(Xtest)\n",
    "accuracy_score(ytest,ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 [ _ /4 marks] \n",
    "\n",
    "What is the **major shortcoming** of standalone decision trees? [ /2 marks] What is the **purpose of creating an ensemble of trees**? [ /2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:The major shortcoming of the standalone decison trees is that it has high variance.\n",
    "To overcome this issue of high variance, we create ensemble of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 [ _ /24 marks]\n",
    "\n",
    "Let's now focus on creating an **ensemble of 500 trees**. For this question we'll consider Bagging (Bootstrap Aggregation). Follow these steps:\n",
    "* **Step 1**: Create 500 Bootstrap samples (i.e. sample with replacement) from the dataset (specifically, sample from the training data).\n",
    "* **Step 2**: Train a particular tree (`max_depth=4`) on each Bootstrap sample (you'll therefore need 500 trees in total)\n",
    "* **Step 3**: Compute the **overall prediction** of your ensemble on the unseen test set. The overall prediction from each individual test input will come from a vote count from each of the 500 trees. **Report the test accuracy**.\n",
    "\n",
    "To expand on the `voting` point from Step 3: For each test input, each tree will make a certain output prediction. So, for a single test input you'll have 500 votes, and these could be 1 or 0. For the overall prediction for that single test input, you'll count which class (0 or 1) got the most votes.\n",
    "\n",
    "Note: Since this question is meant to be a manually done, you won't get the marks if you use sklearn's `BaggingClassifier` or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979166666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 500 Bootstrap samples from the training set. Fit a tree to each [ /12 marks]\n",
    "# ** your code here **\n",
    "#Decision Tree Classifier\n",
    "dtc = DecisionTreeClassifier(max_depth=4,random_state=0)\n",
    "#Creating 500 Bootstrap samples from the training set\n",
    "#And then fitting them\n",
    "def samples(S):\n",
    "    sample_size=len(S)\n",
    "    a = []\n",
    "    num_iteration = 500\n",
    "    for x in range(num_iteration):\n",
    "        S_new = S.sample(sample_size,random_state=0,replace=True)\n",
    "        dtc.fit(S_new.iloc[:,0:11], S_new.iloc[:,11])\n",
    "        a.append(dtc)\n",
    "    return a\n",
    "\n",
    "#Making data\n",
    "data=pd.concat([Xtrain, ytrain], axis=1)\n",
    "a1 = samples(data)\n",
    "\n",
    "\n",
    "# Make 500 predictions on the test data (unseen by your trees so far). [ /6 marks] \n",
    "# ** your code here **\n",
    "ypr = []\n",
    "for i in range(0,500):\n",
    "    y_pr=a1[i].predict(Xtest)\n",
    "    ypr.append(y_pr)\n",
    "\n",
    "\n",
    "# Finally, compute the overall vote for each prediction (the most votes for a given class wins) [ /4 marks]\n",
    "# Hint: If there's a tie, the common way is to predict the class with the lowest class label\n",
    "# However, it's also ok to use scipy.stats.mode here (this randomly picks tie winners)\n",
    "# ** your code here **\n",
    "ypr_ar=np.array(ypr)\n",
    "#transposing\n",
    "ypr1 = ypr_ar.T\n",
    "#Using stats.mode to get the winner\n",
    "ypr_voting = stats.mode(ypr1, axis=1)\n",
    "yp=ypr_voting[0]\n",
    "\n",
    "\n",
    "# Report (print) the accuracy of your ensemble model on the test set. Use accuracy_score() [ /2 marks]\n",
    "# ** your code here **\n",
    "accuracy_score(ytest, yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 [ _ /6 marks]\n",
    "\n",
    "Finally, let's consider AdaBoost. Here, each tree (a stump) is trained sequentially and relies on the previous tree for its training data (which was re-sampled, and this was influenced by the sample weight changes as a result of incorrect predictions from the previous tree). \n",
    "\n",
    "Create an `AdaBoostClassifier` object with `base_estimator = DecisionTreeClassifier(max_depth=4)`, `n_estimators=500`, `learning_rate=0.1`. Fit to the training data, and compute (and report) the test accuracy. You can use `accuracy_score()` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9083333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an AdaBoostClassifier object with the specified arguments [ /2 marks]\n",
    "# ****** your code here ******\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),random_state=0,n_estimators=500, learning_rate=0.1)\n",
    "\n",
    "# Fit to the training data and compute the test predictions [ /2 marks]\n",
    "# ****** your code here ******\n",
    "abc.fit(Xtrain,ytrain)\n",
    "ypred=abc.predict(Xtest)\n",
    "\n",
    "# Compute and report the test accuracy [ /2 marks]\n",
    "# ****** your code here ******\n",
    "accuracy_score(ytest,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 [ _ /4 marks]\n",
    "\n",
    "Finally, compare the test accuracies of the models considered so far: the optimal max_depth Decision Tree, the bagged ensemble of 500 trees, and the AdaBoost ensemble of 500 trees. Which performed worst? Which performed best? Do the results agree with our intuition from the Lecture?\n",
    "\n",
    "Hint: One intuition you could use from the lecture was that ensemble methods have **lower variance** than a single tree (i.e. less prone to overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: The normal decision tree classifier in question 2.2 is performing worst as compare to other models.\n",
    "While the Ada Boost Classifier in question 3.2 performs the best among all the classifiers.\n",
    "And yes, results agree with the intuition from lecture as we can see the changes in the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
